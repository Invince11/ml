{"componentChunkName":"component---node-modules-lekoarts-gatsby-theme-minimal-blog-core-src-templates-post-query-tsx","path":"/diarization-bad-bad-audio-detector","result":{"data":{"post":{"__typename":"MdxPost","slug":"/diarization-bad-bad-audio-detector","title":"Diarization & BAD (Bad Audio Detector)","date":"21.07.2020","tags":[{"name":"Speech ML","slug":"speech-ml"},{"name":"Speaker Diarization","slug":"speaker-diarization"},{"name":"Diarization Data Simulation","slug":"diarization-data-simulation"}],"description":null,"body":"function _extends() { _extends = Object.assign || function (target) { for (var i = 1; i < arguments.length; i++) { var source = arguments[i]; for (var key in source) { if (Object.prototype.hasOwnProperty.call(source, key)) { target[key] = source[key]; } } } return target; }; return _extends.apply(this, arguments); }\n\nfunction _objectWithoutProperties(source, excluded) { if (source == null) return {}; var target = _objectWithoutPropertiesLoose(source, excluded); var key, i; if (Object.getOwnPropertySymbols) { var sourceSymbolKeys = Object.getOwnPropertySymbols(source); for (i = 0; i < sourceSymbolKeys.length; i++) { key = sourceSymbolKeys[i]; if (excluded.indexOf(key) >= 0) continue; if (!Object.prototype.propertyIsEnumerable.call(source, key)) continue; target[key] = source[key]; } } return target; }\n\nfunction _objectWithoutPropertiesLoose(source, excluded) { if (source == null) return {}; var target = {}; var sourceKeys = Object.keys(source); var key, i; for (i = 0; i < sourceKeys.length; i++) { key = sourceKeys[i]; if (excluded.indexOf(key) >= 0) continue; target[key] = source[key]; } return target; }\n\n/* @jsx mdx */\nvar _frontmatter = {\n  \"title\": \"Diarization & BAD (Bad Audio Detector)\",\n  \"date\": \"2020-07-21T00:00:00.000Z\",\n  \"tags\": [\"Speech ML\", \"Speaker Diarization\", \"Diarization Data Simulation\"],\n  \"author\": \"anirudhdagar\"\n};\n\nvar makeShortcode = function makeShortcode(name) {\n  return function MDXDefaultShortcode(props) {\n    console.warn(\"Component '\" + name + \"' was not imported, exported, or provided by MDXProvider as global scope\");\n    return mdx(\"div\", props);\n  };\n};\n\nvar layoutProps = {\n  _frontmatter: _frontmatter\n};\nvar MDXLayout = \"wrapper\";\nreturn function MDXContent(_ref) {\n  var components = _ref.components,\n      props = _objectWithoutProperties(_ref, [\"components\"]);\n\n  return mdx(MDXLayout, _extends({}, layoutProps, props, {\n    components: components,\n    mdxType: \"MDXLayout\"\n  }), mdx(\"span\", {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"711px\"\n    }\n  }, \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"64.99999999999999%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAACp0lEQVQ4y1WTyW7TUBSG8x5s2MAKicdgw7JPwHMgISFYwAOwoytEJVQaOqBUVUuTCpJmbpqEZnImO3ZsZ56ciXzca5TQLH6f6+Nzz/3P/X97ptMxazjOaAvlUgOlqGHoNk3DxrbaW9/Xe+738NxPyji5h+ODGB8/XPLdG+Py7JZoKMdkMhS1suEQZzLY2ivhuc9uOZ/A0mEl4nTUpVGtYqgCRgO9adHpD2iImCuU6A2GrIDZfLrFcsNwJqLaapHWLQGT/XM/oYJCqqbRaeuoOT/ZyDG310dY1QhGOYxR+knXrjOdOf8ZysdYABbsRrI8F2Pu7AV49n6XnYMwL05SpOKnHL5+zJeXDzl584hvb59w8u4pX189IBv6zPwP7vgbhuPxANtuohsaSqVMLv+bcDRMpapQrVfo9dtUNJWarpLN3mA16vR0Daus4Izl6MvNtXlmswmtlklUNEgkoiQFopFrYuI9lUqIdYhCMU+i3OS2aqKoBvpNBi2ZRk+k0ZQSJaXAcNBF9vKsL3SxmLoJifnCwbJ0UZgXrFVMs4FaK7loNGq02xZ226TTawnBNMrlEsNhb1sUaYe5ULei1DnzBfGdBNj7dMSh94KD/TOUUpXVav7vwLmzwXI5E/mFu96IIhV2IYot2yaeyBC4uiYaT5PNFIlFUlimKaZwXBby8DUJyazf77hx48PeeERrOGQiLngpvGbXa1hKmWXTFp50hAOWjEY9crk7rq784r4jXFycEwqFiMdj4j1KMBhEUYp4RpMR0bpJWLUwDWHeqyTtQJLurzSddJGuppO9y5DJpLi8/IHX68Xn87nw+/0EAgFOT0/dfD5/h2csaDetJrZQWqrdkmtDxzYN2v0WTSGIISwix6vXqy4LKYIq/iBd2EjTaq4wcmxZ8xcNWrGMYgiupQAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Can't See? Something went wrong!\",\n    \"title\": \"Can't See? Something went wrong!\",\n    \"src\": \"/static/6790fa68bbb7e4d7b9b0e169877242f1/76a04/speaker_diarization_vernacular.png\",\n    \"srcSet\": [\"/static/6790fa68bbb7e4d7b9b0e169877242f1/5243c/speaker_diarization_vernacular.png 240w\", \"/static/6790fa68bbb7e4d7b9b0e169877242f1/ab158/speaker_diarization_vernacular.png 480w\", \"/static/6790fa68bbb7e4d7b9b0e169877242f1/76a04/speaker_diarization_vernacular.png 711w\"],\n    \"sizes\": \"(max-width: 711px) 100vw, 711px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \" NOTE: This is a Draft version. There can be typos and unreferenced quotes.\")), mdx(\"h3\", null, \"Diarization Introduction - Who spoke when?\"), mdx(\"p\", null, \"Speech processing systems can be broadly split into two categories:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Speech Recognition\"), \": The contents of speech in the audio are detected. \"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Speaker Recognition\"), \": Task of identifying speakers in a conversation.\")), mdx(\"p\", null, \"Speaker Diarization falls in the second category of speech processing where it is required to identify the speaker along with identification of the boundary/frame of the speech spoken by a particular speaker.\"), mdx(\"blockquote\", null, mdx(\"p\", {\n    parentName: \"blockquote\"\n  }, \"Speaker diarization is the process of recognizing \\u201Cwho spoke when.\\u201D \")), mdx(\"p\", null, \"In an audio conversation with multiple speakers (phone calls, conference calls, dialogs etc.), the Diarization API identifies the speaker at precisely the time they spoke during the conversation.\"), mdx(\"hr\", null), mdx(\"p\", null, \" \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TODO: Fix Audio Examples.\"), \" \"), mdx(\"p\", null, \"Below are two audio files from calls recorded at a customer care center, where the agent is involved in a one-to-one dialog with the customer. A state-of-the-art automatic speech recognition transcription is also attached for reference along with the timestamps of Customer and Agent.\"), mdx(\"p\", null, \"One of these is about  \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TODO\"), \" <\", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"blah blah blah\"), \">\", \", other is about\"), mdx(\"p\", null, \"This can be very hard sometimes also (See \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://vai-diarization.s3.ap-south-1.amazonaws.com/1573539785.2420125.002.wav\"\n  }), \"this\"), \")\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TODO:\"), \" Add transcription as well as speaker timestamps to audios below just like in the cover image of blog.\"), mdx(\"div\", null, mdx(\"audio\", {\n    controls: \"controls\"\n  }, mdx(\"source\", {\n    src: \"https://vai-diarization.s3.ap-south-1.amazonaws.com/1573539792.52506.003.wav\"\n  })), mdx(\"span\", {\n    className: \"abstract\",\n    style: {\n      \"position\": \"relative\",\n      \"top\": \"-1.2em\",\n      \"paddingLeft\": \"1em\"\n    }\n  }, mdx(\"b\", null, \" -> \"), \" \\u2003 [\", mdx(\"b\", null, \"Transcription\"), \": Blah Blah ... ] [\", mdx(\"b\", null, \"Diarization Tag\"), \": AGENT: [(4.111, 5.376), (8.991, 12.213)], CUSTOMER: [(6.951, 7.554)]\")), mdx(\"div\", null, mdx(\"audio\", {\n    controls: \"controls\"\n  }, mdx(\"source\", {\n    src: \"https://vai-diarization.s3.ap-south-1.amazonaws.com/1573539653.2419968.006.wav\"\n  })), mdx(\"span\", {\n    className: \"abstract\",\n    style: {\n      \"position\": \"relative\",\n      \"top\": \"-1.2em\",\n      \"paddingLeft\": \"1em\"\n    }\n  }, mdx(\"b\", null, \" -> \"), \" \\u2003 [\", mdx(\"b\", null, \"Transcription\"), \": Blah Blah ... ] [\", mdx(\"b\", null, \"Diarization Tag\"), \": AGENT: [(4.111, 5.376), (8.991, 12.213)], CUSTOMER: [(6.951, 7.554)]\")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TODO:\"), \" Add clustering image.\\nThe image shows the cluster generated based on the speech pattern and precise time the speaker participated in the conversation.\"), mdx(\"p\", null, \"Speaker recognition/diarization is the identification of an individual person based on characteristics found in the unique voice qualities.\"), mdx(\"h5\", null, \"What Diarization is NOT ?\"), mdx(\"p\", null, \"There is a fine line between speaker diarization and other related speech processing tasks.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Diarization != Speaker Change Detection\"), \" : Diarization systems spit a label, whenever a new speaker appears and if the same speaker comes again, it provides the same label. However, in speaker change detection no such labels are given, only the boundary of change is considered for prediction.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Diarization != Speaker Identification\"), \" : The goal is not to learn the voice prints of any known speaker. Speakers' are not registered before running the model.\"))), mdx(\"hr\", null), mdx(\"h3\", null, \"Motivation?\"), mdx(\"figure\", null, \"\\n  \", mdx(\"span\", _extends({\n    parentName: \"figure\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"711px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"64.99999999999999%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAANCAYAAACpUE5eAAAACXBIWXMAAAsSAAALEgHS3X78AAACp0lEQVQ4y1WTyW7TUBSG8x5s2MAKicdgw7JPwHMgISFYwAOwoytEJVQaOqBUVUuTCpJmbpqEZnImO3ZsZ56ciXzca5TQLH6f6+Nzz/3P/X97ptMxazjOaAvlUgOlqGHoNk3DxrbaW9/Xe+738NxPyji5h+ODGB8/XPLdG+Py7JZoKMdkMhS1suEQZzLY2ivhuc9uOZ/A0mEl4nTUpVGtYqgCRgO9adHpD2iImCuU6A2GrIDZfLrFcsNwJqLaapHWLQGT/XM/oYJCqqbRaeuoOT/ZyDG310dY1QhGOYxR+knXrjOdOf8ZysdYABbsRrI8F2Pu7AV49n6XnYMwL05SpOKnHL5+zJeXDzl584hvb59w8u4pX189IBv6zPwP7vgbhuPxANtuohsaSqVMLv+bcDRMpapQrVfo9dtUNJWarpLN3mA16vR0Daus4Izl6MvNtXlmswmtlklUNEgkoiQFopFrYuI9lUqIdYhCMU+i3OS2aqKoBvpNBi2ZRk+k0ZQSJaXAcNBF9vKsL3SxmLoJifnCwbJ0UZgXrFVMs4FaK7loNGq02xZ226TTawnBNMrlEsNhb1sUaYe5ULei1DnzBfGdBNj7dMSh94KD/TOUUpXVav7vwLmzwXI5E/mFu96IIhV2IYot2yaeyBC4uiYaT5PNFIlFUlimKaZwXBby8DUJyazf77hx48PeeERrOGQiLngpvGbXa1hKmWXTFp50hAOWjEY9crk7rq784r4jXFycEwqFiMdj4j1KMBhEUYp4RpMR0bpJWLUwDWHeqyTtQJLurzSddJGuppO9y5DJpLi8/IHX68Xn87nw+/0EAgFOT0/dfD5/h2csaDetJrZQWqrdkmtDxzYN2v0WTSGIISwix6vXqy4LKYIq/iBd2EjTaq4wcmxZ8xcNWrGMYgiupQAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Can't See? Something went wrong!\",\n    \"title\": \"Can't See? Something went wrong!\",\n    \"src\": \"/static/6790fa68bbb7e4d7b9b0e169877242f1/76a04/speaker_diarization_vernacular.png\",\n    \"srcSet\": [\"/static/6790fa68bbb7e4d7b9b0e169877242f1/5243c/speaker_diarization_vernacular.png 240w\", \"/static/6790fa68bbb7e4d7b9b0e169877242f1/ab158/speaker_diarization_vernacular.png 480w\", \"/static/6790fa68bbb7e4d7b9b0e169877242f1/76a04/speaker_diarization_vernacular.png 711w\"],\n    \"sizes\": \"(max-width: 711px) 100vw, 711px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \"), \"\\n  \", mdx(\"figcaption\", {\n    parentName: \"figure\"\n  }, mdx(\"b\", {\n    parentName: \"figcaption\"\n  }, mdx(\"center\", {\n    parentName: \"b\"\n  }, \"Fig 1.: ASR using Diarization tags to understand and segregate transcription.\")))), mdx(\"p\", null, \"With the rise of speech recognition systems both in terms of scale and accuracy, the ability to process audio of multiple speakers is crucial and has become quintessential to understand speech today.\"), mdx(\"p\", null, \"As illustrated in \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Fig 1.\"), \" above, information gained through diarization helps in enriching and improving Spoken Language Understanding (SLU) based on the Automatic Speech Recognition (ASR) transcription, which can valuable for downstream applications such as analytics for call-center transcription and meeting transcription etc.\"), mdx(\"p\", null, \"Other than this, we at \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://vernacular.ai/\"\n  }), \"Vernacular.ai\"), \" work on \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"Call Center Automation (CCA)\")), \" at the very core, where we can definitely utilize speaker indexing information gained after diarization. \"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"How?\")), mdx(\"p\", null, \"Let me explain the goal of every customer care call support service if you are not aware. The ultimate aim is to provide best in class service/help to the customers of the respective company. A measure of the quality of service is based on the assessment of the \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"AGENT\"), \" (Call representative at customer care center).\"), mdx(\"p\", null, \"During the quality check phase, an \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"AGENT's\"), \" performance is scored on mutiple parameters, such as (but not limited to):\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Whether the agent was patient enough listeing to the customer or was rushing on the call,\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Whether she/he was rude to the lead at any time or not,\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Whether she/he used the proper language.\")), mdx(\"p\", null, \"Other applications involve:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Information retrieval from broadcast news.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Generating notes/minutes of meetings.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Turn-taking analysis of telephone conversations.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Call center Data analysis\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Court houses & Parliaments.\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Broadcast News(TV and Radio)\")), mdx(\"p\", null, \"In most of the conversations that our algorithms will need to work with, people will interrupt each other, overtalk etc. and cutting the audio between sentences won\\u2019t be a trivial task.\"), mdx(\"p\", null, \"For such occasions, identifying different speakers and finally connecting different sentences under the same speaker is a critical task.\"), mdx(\"p\", null, \"Speaker Diarization is the solution for those problems.\"), mdx(\"h3\", null, \"DIHARD? Is it Hard?\"), mdx(\"p\", null, \"This is not \", mdx(\"u\", null, \"1+2=3\"), \". This is \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Diarization\"), \" and \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"IT IS HARD\")), \". One can say that it is one of the toughest ML problems intrinsically high on complexity, even for a human-being, in certain conditions. \"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"But Why???\")), mdx(\"p\", null, \"Real world audios are not always \", mdx(\"em\", {\n    parentName: \"p\"\n  }, \"sunshine and rainbows\"), \". They come with infinite complexities.\\nTo name a few:\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Speakers are discovered dynamically. Although as you'll see later, in our case we only have 2 speakers, a fixed number.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Most conversational audios are highly interactive and have speaker overtalking (overlap).\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Sometimes the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"audio is noisy\"), \":\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"People \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"talking\"), \" in the \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"background\"), \".\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"The microphone picking up speakers' \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"environment noises\"), \" (roadside noises, industrial machinery noise, music in the background etc.).\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"For telephony based audios, \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"connection may be weak\"), \" at times, leading to:\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Audio being \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"dropped/corrupted\"), \" in some parts(TODO: Find a better word)\"), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"Static\"), \"  or just some \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"buzzing\"), \" noise creeping in the conversation and finding it's way into the audio recording.\")))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Believe me\"), \", this is not the end of many problems for diarization!\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Maybe in a conference call with multiple speakers, even if the audio is clear, the \", mdx(\"strong\", {\n    parentName: \"li\"\n  }, \"difference can be very subtle\"), \" between the speakers, and it is not always possible to identify/label the correct speaker for a particular timestamp/duration. \")), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Ok, so that's it?\")), mdx(\"p\", null, \"If I have not made my point clear about the complexity of the problem, yet, then I'll express my message through this legendary meme.\"), mdx(\"figure\", null, \"\\n  \", mdx(\"span\", _extends({\n    parentName: \"figure\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"648px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"56.25%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAABAACBf/EABYBAQEBAAAAAAAAAAAAAAAAAAMBAv/aAAwDAQACEAMQAAABCs7CXdyrF//EABkQAQADAQEAAAAAAAAAAAAAAAIAAQMEEv/aAAgBAQABBQINXG6K9FTmmmYrOf/EABYRAQEBAAAAAAAAAAAAAAAAAAAREv/aAAgBAwEBPwGxp//EABYRAQEBAAAAAAAAAAAAAAAAAAAREv/aAAgBAgEBPwGVh//EAB4QAAIBAwUAAAAAAAAAAAAAAAABEQIQMRIhQWGx/9oACAEBAAY/AsuF2J6tifCOCqKVi3//xAAaEAEAAwEBAQAAAAAAAAAAAAABABExIZGB/9oACAEBAAE/Ici67iW9DHlvs1l+p230xUAlj2IT/9oADAMBAAIAAwAAABCbP//EABgRAAMBAQAAAAAAAAAAAAAAAAABESFx/9oACAEDAQE/EJSTWcn/xAAYEQACAwAAAAAAAAAAAAAAAAAAEQEhcf/aAAgBAgEBPxB5N0aP/8QAHBABAAICAwEAAAAAAAAAAAAAAQARIWExQVGx/9oACAEBAAE/EGhpyhQ6M/IJxcWA8Gfe5zIXDVRp3HhBpol9SzOADhpFN0Zn/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Can't See? Something went wrong!\",\n    \"title\": \"Can't See? Something went wrong!\",\n    \"src\": \"/static/411499dae7959f9f0715210b35ddbefc/1f16e/diarization_hard.jpg\",\n    \"srcSet\": [\"/static/411499dae7959f9f0715210b35ddbefc/46946/diarization_hard.jpg 240w\", \"/static/411499dae7959f9f0715210b35ddbefc/55489/diarization_hard.jpg 480w\", \"/static/411499dae7959f9f0715210b35ddbefc/1f16e/diarization_hard.jpg 648w\"],\n    \"sizes\": \"(max-width: 648px) 100vw, 648px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \"), \"\\n  \", mdx(\"figcaption\", {\n    parentName: \"figure\"\n  }, mdx(\"b\", {\n    parentName: \"figcaption\"\n  }, mdx(\"center\", {\n    parentName: \"b\"\n  }, \"Fig 2.: Diarization is hard!\")))), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"More Problems?\")), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"All the problems stated above are considering that preprocessing steps like VAD/SAD worked perfectly, which you may have guessed, are obviously not 100% accurate.\"), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"  What is this preprocessing step?\"), mdx(\"ul\", {\n    parentName: \"li\"\n  }, mdx(\"li\", {\n    parentName: \"ul\"\n  }, \"Voice Activity Detection (VAD) or Speech Activity Detection (SAD) is a widely used technique before running a typical diariaztion api based on the clustering of speaker embeddings. The objective of VAD/SAD is to get rid of all non-speech regions.\")))), mdx(\"hr\", null), mdx(\"h3\", null, \"Approaching the problem\"), mdx(\"p\", null, \"Keeping in mind the complexity and hardness of the problem, multiple approaches have been devised over the years to tackle diarization, like Hidden Markov Model (HMM)/ Gaussian Mixture Model (GMM) based speaker diarization systems, Embedding Clustering based methods and End2End Neural methods etc.\"), mdx(\"p\", null, \"As stated in \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"(TODO: cite \", mdx(\"a\", _extends({\n    parentName: \"em\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1909.06247.pdf\"\n  }), \"this\"), \" paper)\")), \" , a pipeline involving x-vector or d-vector clustering-based system is commonly used for speaker diarization.\"), mdx(\"p\", null, \"The aim of speaker clustering is to put together all the segments that belong to the same acoustic source in a recording. These don't utilize any prior information of the speaker ID or the number of speakers in the recording. We'll be covering a typical embedding-clustering based approach in detail in the latter sections of the blog.\"), mdx(\"p\", null, \"However, speaker diarization systems which combine the two tasks in a unified framework are gaining popularity recently. Fig 3 briefly summarizes the End2End idea. Due to the increased amounts of data being avaialable, joint End2End modeling methods are slowly taking over older approaches across ML domains in an effort to alleviate the complex preparation processes.\"), mdx(\"p\", null, \"One example is the \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, mdx(\"em\", {\n    parentName: \"strong\"\n  }, \"EEND End2End Neural Diarization\")), \" (\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"TODO: ADD CITATION\"), \") proposed recently showing some promise in regards to solving these complex steps jointly.\"), mdx(\"figure\", null, \"\\n  \", mdx(\"span\", _extends({\n    parentName: \"figure\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"637px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"33.75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAHABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAIDBf/EABQBAQAAAAAAAAAAAAAAAAAAAAD/2gAMAwEAAhADEAAAAd2AWA//xAAWEAEBAQAAAAAAAAAAAAAAAAABABD/2gAIAQEAAQUChz//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAUEAEAAAAAAAAAAAAAAAAAAAAQ/9oACAEBAAY/An//xAAZEAADAQEBAAAAAAAAAAAAAAAAARExQaH/2gAIAQEAAT8hdWvwh0Vmn//aAAwDAQACAAMAAAAQ88//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/ED//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAYEAEBAQEBAAAAAAAAAAAAAAABEQAhMf/aAAgBAQABPxBjRC4F8gt8t1qQ3oyb/9k=')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Can't See? Something went wrong!\",\n    \"title\": \"Can't See? Something went wrong!\",\n    \"src\": \"/static/0f5c773e8a5432d2c2d4489de1a698e1/60f36/EEND.jpg\",\n    \"srcSet\": [\"/static/0f5c773e8a5432d2c2d4489de1a698e1/46946/EEND.jpg 240w\", \"/static/0f5c773e8a5432d2c2d4489de1a698e1/55489/EEND.jpg 480w\", \"/static/0f5c773e8a5432d2c2d4489de1a698e1/60f36/EEND.jpg 637w\"],\n    \"sizes\": \"(max-width: 637px) 100vw, 637px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \"), \"\\n  \", mdx(\"figcaption\", {\n    parentName: \"figure\"\n  }, mdx(\"b\", {\n    parentName: \"figcaption\"\n  }, mdx(\"center\", {\n    parentName: \"b\"\n  }, \"Fig 3.: An End to End approach diarization system.\")))), mdx(\"p\", null, \"We decided to try out multiple approaches and model experiments explained in the sections below.\"), mdx(\"hr\", null), mdx(\"h3\", null, \"Defining Our Problem\"), mdx(\"h4\", null, \"What constitutes a robust diarization system?\"), mdx(\"p\", null, \"Our model should be powerful enough to capture global speaker characteristics in addition to local speech activity dynamics.\\nSystems, that are able to accurately handle \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"highly interactive\"), \" and \", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"overlapping speech\"), \" from a range of conversational domains, while being resilient to variation in recording equipment, recording environment, reverberation, ambient noise, number of speakers, and\\nspeaker demographics. \"), mdx(\"hr\", null), mdx(\"h3\", null, \"Pipeline (Anatomy of Processes Involved)\"), mdx(\"figure\", null, \"\\n  \", mdx(\"span\", _extends({\n    parentName: \"figure\"\n  }, {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }), \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"19.166666666666664%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAECAYAAACOXx+WAAAACXBIWXMAABcSAAAXEgFnn9JSAAAAqUlEQVQY022P2wqEMAxE/f8/6ZN/0gdBpWC9Q71bQVDwMpIsfVh3B0IGcjIkHl66rgv3fbPf9x1hGEJKiXVdfxhXTuQ93/dRVRWapkFd17xIPcsyLMuCsix5bq2F1hppmmKeZ/R9j3EcmaF5URTYtg2eEIJBF0iwUgpRFGGaJobzPMcwDEiShNm2bbm6rmOGwo0xn8D3y3S2e+k8T8RxjCAIcBzHF/PPkx7Eci+mJ4wOoAAAAABJRU5ErkJggg==')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Can't See? Something went wrong!\",\n    \"title\": \"Can't See? Something went wrong!\",\n    \"src\": \"/static/d55bc75c1898ac3e97f1fe28ea3d1834/7d769/diarization_pipeline.png\",\n    \"srcSet\": [\"/static/d55bc75c1898ac3e97f1fe28ea3d1834/5243c/diarization_pipeline.png 240w\", \"/static/d55bc75c1898ac3e97f1fe28ea3d1834/ab158/diarization_pipeline.png 480w\", \"/static/d55bc75c1898ac3e97f1fe28ea3d1834/7d769/diarization_pipeline.png 960w\", \"/static/d55bc75c1898ac3e97f1fe28ea3d1834/acdd1/diarization_pipeline.png 1150w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \"), \"\\n  \", mdx(\"figcaption\", {\n    parentName: \"figure\"\n  }, mdx(\"b\", {\n    parentName: \"figcaption\"\n  }, mdx(\"center\", {\n    parentName: \"b\"\n  }, \"Fig 5.: A typical diarization pipeline.\")))), mdx(\"p\", null, \"Since we have a more focused problem, for evaluating customer care center call audios, the number of speakers for our use case was fixed at two.\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"VAD\"), \" \\u2014 Use VAD to remove noise and non speech. \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/wiseman/py-webrtcvad\"\n  }), \"WebRTC VAD\"), \", WebRTC is the model we employ during our experiments. Raw audios are split into frames with specific duration (30 ms in our case). For each input frame, WebRTC generates output 1 or 0, where 1 denotes speech and 0 denotes nonspeech. An optional setting of WebRTC is the aggressive mode, an integer between 0 and 3. 0 is the least aggressive about filtering out nonspeech while 3 is the most aggressive. These VAD/SAD models have their own respective struggles with and set of problems.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Speech Segmentation \\u2014 Extract short segments(sliding window) from audio & Run LSTM network to produce D vectors for each sliding window.\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Embedding Extraction(Recognition part) \\u2014 For each segment aggregate the D vector belong to that segment to produce segment wise embeddings (Attach the d-vector to sliding window)\")), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Clustering (Cluster the segments) \\u2014 Finally cluster the segment wise embedding to produce diarization results. Determine the number of speakers with each speakers time stamps.\"))), mdx(\"p\", null, \"The first step is usually dedicated to speech activity detection, where the objective is to get rid of all non-speech regions.\\n\", mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"WebRTC VAD:\"), \"\\nWebRTC is the official VAD baseline for Track2. Raw audios are split into frames with 20ms duration. For each input frame, WebRTC generates output 1 or 0, where 1 denotes speech and 0 denotes nonspeech. An optional setting of WebRTC is the aggressive mode, an integer between 0 and 3. 0 is the least aggressive about filtering out nonspeech while 3 is the most aggressive.\"), mdx(\"p\", null, \"Then, speaker change detection aims at segmenting speech regions into homogeneous segments. The subsequent clustering step tries to group those speech segments according to the identity of the speaker. Finally, an optional supervised classification step may be applied to actually identity every speaker cluster in a supervised way.\"), mdx(\"p\", null, \"Looking at the final performance of the system is usually not enough for diagnostic purposes. In particular, it is often necessary to evaluate the performance of each module separately to identify their strenght and weakness, or to estimate the influence of their errors on the complete pipeline.\"), mdx(\"p\", null, \"Here, we provide the list of metrics that were implemented in pyannote.metrics with that very goal in mind.\"), mdx(\"p\", null, \"Because manual annotations cannot be precise at the audio sample level, it is common in speaker diarization research to remove from evaluation a 500ms collar around each speaker turn boundary (250ms before and after). Most of the metrics available in pyannote.metrics support a collar parameter, which defaults to 0.\"), mdx(\"p\", null, \"Moreover, though audio files can always be processed entirely (from beginning to end), there are cases where reference annotations are only available for some regions of the audio files. All metrics support the provision of an evaluation map that indicate which part of the audio file should be evaluated.\"), mdx(\"p\", null, \"Detection\\nThe two primary metrics for evaluating speech activity detection modules are detection error rate and detection cost function.\"), mdx(\"p\", null, \"Detection error rate (not to be confused with diarization error rate) is defined as:\"), mdx(\"p\", null, \"detection error rate=false alarm+missed detectiontotal\\nwhere false alarm is the duration of non-speech incorrectly classified as speech, missed detection is the duration of speech incorrectly classified as non-speech, and total is the total duration of speech in the reference.\"), mdx(\"p\", null, \"Alternately, speech activity module output may be evaluated in terms of detection cost function, which is defined as:\"), mdx(\"p\", null, \"detection cost function=0.25\\xD7false alarm rate+0.75\\xD7miss rate\\nwhere false alarm rate is the proportion of non-speech incorrectly classified as speech and miss rate is the proportion of speech incorrectly classified as non-speech.\"), mdx(\"p\", null, \"Additionally, detection may be evaluated in terms of accuracy (proportion of the input signal correctly classified), precision (proportion of detected speech that is speech), and recall (proporton of speech that is detected).\"), mdx(\"h4\", null, \"Resemblyzer + D-Vector\"), mdx(\"p\", null, \"The state-of-the-art speaker diarization systems use agglomerative hierarchical clustering (AHC) which performs the clustering of previously learned neural embeddings. While the clustering approach attempts to identify speaker clusters, the AHC\\nalgorithm does not involve any further learning. In this paper,\\nwe propose a novel algorithm for hierarchical clustering which\\ncombines the speaker clustering along with a representation\\nlearning framework. The proposed approach is based on principles of self-supervised learning where the self-supervision is\\nderived from the clustering algorithm. The representation learning network is trained with a regularized triplet loss using the\\nclustering solution at the current step while the clustering algorithm uses the deep embeddings from the representation learning step. By combining the self-supervision based representation learning along with the clustering algorithm, we show\\nthat the proposed algorithm improves significantly (29% relative improvement) over the AHC algorithm with cosine similarity for a speaker diarization task on CALLHOME dataset. In\\naddition, the proposed approach also improves over the stateof-the-art system with PLDA affinity matrix with 10% relative\\nimprovement in DER.\"), mdx(\"h4\", null, \"Soft Clustering?\"), mdx(\"h4\", null, \"DER (Metrics)\"), mdx(\"ol\", {\n    \"start\": 3\n  }, mdx(\"li\", {\n    parentName: \"ol\"\n  }, \"Evaluation Metrics\\nDiarization Error Rate (DER) is the metric used to measure the performance of speaker diarization systems. It is measured as the fraction of time that is not attributed correctly to a speaker or non-speech.\")), mdx(\"p\", null, \"The DER is composed of the following three errors:\"), mdx(\"p\", null, \"Speaker Error: It is the percentage of scored time that a speaker ID is assigned to the wrong speaker. Speaker error is mainly a diarization system error (i.e., it is not related to speech/non-speech detection.) It also does not take into account the overlap speeches not detected.\"), mdx(\"p\", null, \"False Alarm: It is the percentage of scored time that a hypothesized speaker is labelled as a non-speech in the reference. The false alarm error occurs mainly due to the the speech/non-speech detection error (i.e., the speech/non-speech detection considers a non-speech segment as a speech segment). Hence, false alarm error is not related to segmentation and clustering errors.\"), mdx(\"p\", null, \"Missed Speech: It is the percentage of scored time that a hypothesized non-speech segment corresponds to a reference speaker segment. The missed speech occurs mainly due to the the speech/non-speech detection error (i.e., the speech segment is considered as a non-speech segment). Hence, missed speech is not related to segmentation and clustering errors.\"), mdx(\"p\", null, \"DER=SpeakerError+FalseAlarm+MissSpeech\"), mdx(\"p\", null, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://pyannote.github.io/pyannote-metrics/reference.html\"\n  }), \"https://pyannote.github.io/pyannote-metrics/reference.html\")), mdx(\"p\", null, \"Moreover, the majority of this work has\\nevaluated systems using a modified version of DER in which\\nspeech within 250 ms of reference boundaries and overlapped\\nspeech are excluded from scoring (Typical Practice).  As short segments such as\\nbackchannels and overlapping speech are both common in conversation, this may have resulted in an over-optimistic assessment of performance even within these domains\"), mdx(\"hr\", null), mdx(\"h4\", null, \"Resegmentation\"), mdx(\"p\", null, \"As stated earlier Speaker diarization consists of automatically partitioning an input audio stream into homogeneous segments (segmentation) and assigning these segments to the same speaker (speaker clustering).\"), mdx(\"p\", null, mdx(\"strong\", {\n    parentName: \"p\"\n  }, \"Is it possible to resegment these assignments post clustering? If yes will that help?\")), mdx(\"p\", null, \"Simply put, we have a condition to be improved, a difficulty to be eliminated, or a troubling question that exists for resegmentation in practice that points to the need for meaningful understanding and deliberate investigation. \"), mdx(\"hr\", null), mdx(\"h5\", null, \"UIS-RNN\"), mdx(\"p\", null, \"We also experimented with UIS-RNN, the current state of the art neural system for Speaker Diarization. Converting data to UIS Style format involves a set of preprocessing steps as described below.\"), mdx(\"p\", null, \"But a caveat with UIS-RNN is that it is a very data heavy model and requires training on is fed to this model for training to learn and form the hypothesis. On Realizing the limited amount of tagged data we had for UIS-RNN, we worked on simulating datasets for Speaker Diarization which in itself comes with some challenges. It is not \"), mdx(\"h5\", null, \"Simulated Data Generation\"), mdx(\"span\", {\n    \"className\": \"gatsby-resp-image-wrapper\",\n    \"style\": {\n      \"position\": \"relative\",\n      \"display\": \"block\",\n      \"marginLeft\": \"auto\",\n      \"marginRight\": \"auto\",\n      \"maxWidth\": \"960px\"\n    }\n  }, \"\\n      \", mdx(\"span\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-background-image\",\n    \"style\": {\n      \"paddingBottom\": \"75%\",\n      \"position\": \"relative\",\n      \"bottom\": \"0\",\n      \"left\": \"0\",\n      \"backgroundImage\": \"url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAPABQDASIAAhEBAxEB/8QAFgABAQEAAAAAAAAAAAAAAAAAAAEF/8QAFQEBAQAAAAAAAAAAAAAAAAAAAAH/2gAMAwEAAhADEAAAAdykVFf/xAAZEAACAwEAAAAAAAAAAAAAAAAAARESIQL/2gAIAQEAAQUCb6NgrqVVEH//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAYEAACAwAAAAAAAAAAAAAAAAAQEQABIf/aAAgBAQAGPwLKLiH/xAAbEAEAAwEAAwAAAAAAAAAAAAABABEhQVFxgf/aAAgBAQABPyEiUP2NMFvuF1psQnohgLo8sNq63s//2gAMAwEAAgADAAAAEEgP/8QAFxEBAAMAAAAAAAAAAAAAAAAAARARMf/aAAgBAwEBPxBLNj//xAAWEQADAAAAAAAAAAAAAAAAAAABEBH/2gAIAQIBAT8QFX//xAAcEAEAAwACAwAAAAAAAAAAAAABABEhQcFRcZH/2gAIAQEAAT8QAzG7unqO4fix1HXlTS7mkSznNjkoKFF+sJNOysu9T//Z')\",\n      \"backgroundSize\": \"cover\",\n      \"display\": \"block\"\n    }\n  })), \"\\n  \", mdx(\"img\", _extends({\n    parentName: \"span\"\n  }, {\n    \"className\": \"gatsby-resp-image-image\",\n    \"alt\": \"Can't See? Something went wrong!\",\n    \"title\": \"Can't See? Something went wrong!\",\n    \"src\": \"/static/7bc05ae08733afa6331002fe26497714/18e3b/sim_dia_data_generator_flow.jpg\",\n    \"srcSet\": [\"/static/7bc05ae08733afa6331002fe26497714/46946/sim_dia_data_generator_flow.jpg 240w\", \"/static/7bc05ae08733afa6331002fe26497714/55489/sim_dia_data_generator_flow.jpg 480w\", \"/static/7bc05ae08733afa6331002fe26497714/18e3b/sim_dia_data_generator_flow.jpg 960w\", \"/static/7bc05ae08733afa6331002fe26497714/60e21/sim_dia_data_generator_flow.jpg 1440w\", \"/static/7bc05ae08733afa6331002fe26497714/69b48/sim_dia_data_generator_flow.jpg 1920w\", \"/static/7bc05ae08733afa6331002fe26497714/7a411/sim_dia_data_generator_flow.jpg 2048w\"],\n    \"sizes\": \"(max-width: 960px) 100vw, 960px\",\n    \"style\": {\n      \"width\": \"100%\",\n      \"height\": \"100%\",\n      \"margin\": \"0\",\n      \"verticalAlign\": \"middle\",\n      \"position\": \"absolute\",\n      \"top\": \"0\",\n      \"left\": \"0\"\n    },\n    \"loading\": \"lazy\"\n  })), \"\\n    \"), mdx(\"ol\", null, mdx(\"li\", {\n    parentName: \"ol\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Datagen stuff \", \"[0/3]\"), mdx(\"ul\", _extends({\n    parentName: \"li\"\n  }, {\n    \"className\": \"contains-task-list\"\n  }), mdx(\"li\", _extends({\n    parentName: \"ul\"\n  }, {\n    \"className\": \"task-list-item\"\n  }), mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"input\", _extends({\n    parentName: \"p\"\n  }, {\n    \"type\": \"checkbox\",\n    \"checked\": true,\n    \"disabled\": true\n  })), \" \", \"Use silences after running through mild VAD\")), mdx(\"li\", _extends({\n    parentName: \"ul\"\n  }, {\n    \"className\": \"task-list-item\"\n  }), mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"input\", _extends({\n    parentName: \"p\"\n  }, {\n    \"type\": \"checkbox\",\n    \"checked\": true,\n    \"disabled\": true\n  })), \" \", \"Find out real-world statistics from BBQ dataset\"), mdx(\"pre\", {\n    parentName: \"li\"\n  }, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \" + [x] Ratio of silent sections vs total duration. 1734 sec / 9955 sec = 0.174\\n + [x] Ratio of overlapping utterances vs total utterance duration. 125 sec / 9955 sec = 0.013\\n\")), mdx(\"p\", {\n    parentName: \"li\"\n  }, \"Link to the notebook -> Here\")), mdx(\"li\", _extends({\n    parentName: \"ul\"\n  }, {\n    \"className\": \"task-list-item\"\n  }), mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"input\", _extends({\n    parentName: \"p\"\n  }, {\n    \"type\": \"checkbox\",\n    \"checked\": true,\n    \"disabled\": true\n  })), \" \", \"Figure out a way to control the amount of overlap in data-gen stuff.\\nHow can I control overlap?\\nscale_overlap :\\nThis allows me to control the maximum possible duration of overlap.\\nbias_overlap :\\nThis allows to control by what % or probability will there be an\\noverlapping segment. Eg: bias_overlap = 0.75 will give 33% chance\\neach time to add overlap. \"))))), mdx(\"p\", null, \"Found a way which can be explained through this:\"), mdx(\"pre\", null, mdx(\"code\", _extends({\n    parentName: \"pre\"\n  }, {}), \"    - Overlap\\n    - Gap\\n    - Dual Audios -> Mono Audios\\n    - VAD\\n        - Agrressive\\n        - Mild\\n    - Our architecture\\n    - Simulated Audio Examples\\n\")), mdx(\"p\", null, \"Stay tuned to our \", mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://vernacular-ai.github.io/ml/rss.xml\"\n  }), \"rss feed\"), \" for\\nupdates.\"), mdx(\"h2\", null, \"References\"), mdx(\"ul\", null, mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"http://leap.ee.iisc.ac.in/sriram/publications/papers/Deep_Self_Supervised_Hierarchical_Clustering_for_Speaker_Diarization.pdf\"\n  }), \"Deep Self-Supervised Hierarchical Clustering for Speaker Diarization\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://ai.googleblog.com/2019/08/joint-speech-recognition-and-speaker.html\"\n  }), \"Joint Speech Recognition and Speaker Diarization via Sequence Transduction\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/wiseman/py-webrtcvad\"\n  }), \"WebRTC VAD\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/2002.12761.pdf\"\n  }), \"DIHARD II is Still Hard: Experimental Results and Discussions\\nfrom the DKU-LENOVO Team\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/pyannote/pyannote-audio\"\n  }), \"PyAnnote Audio\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/resemble-ai/Resemblyzer\"\n  }), \"Resemblyzer\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://github.com/wq2012/awesome-diarization\"\n  }), \"Awesome Diarization\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://www.youtube.com/watch?v=kEcUcfLmIS0\"\n  }), \"Robust Speaker Diarization for Meetings\"))), mdx(\"li\", {\n    parentName: \"ul\"\n  }, mdx(\"p\", {\n    parentName: \"li\"\n  }, mdx(\"a\", _extends({\n    parentName: \"p\"\n  }, {\n    \"href\": \"https://arxiv.org/pdf/1710.10467.pdf\"\n  }), \"GENERALIZED END-TO-END LOSS FOR SPEAKER VERIFICATION\")))));\n}\n;\nMDXContent.isMDXComponent = true;","excerpt":" NOTE: This is a Draft version. There can be typos and unreferenced quotes. Diarization Introduction - Who spoke when? Speech processing…","timeToRead":10,"banner":null}},"pageContext":{"slug":"/diarization-bad-bad-audio-detector","formatString":"DD.MM.YYYY"}},"staticQueryHashes":["318001574","3787687951","3787687951"]}